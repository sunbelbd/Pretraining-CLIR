# Cross-lingual Language Model Pretraining for Retrieval

## Introduction
This repository contains code that supports experiments in our WWW 2021 
paper "Cross-lingual Language Model Pretraining for Retrieval". 
Note that this is the PaddlePaddle version of the implementation, 
which is largely motivated and modified from the XLM codebase by Facebook AI Research, 
and the Transformers library by HuggingFace.  

There is also a Pytorch version, which is available on https://github.com/PxYu/Pretraining-CLIR.

## Usage

## Citation
If you find our code or paper useful, please consider citing our work.
@inproceedings{yu2021cross,
  title={Cross-lingual Language Model Pretraining for Retrieval},
  author={Yu, Puxuan and Fei, Hongliang and Li, Ping},
  booktitle={Proceedings of the Web Conference 2021},
  pages={1029--1039},
  year={2021}
}
