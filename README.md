# Cross-lingual Language Model Pretraining for Retrieval

## Introduction
This repository contains code that supports experiments in our WWW 2021 
paper "Cross-lingual Language Model Pretraining for Retrieval". 
Note that this is the PaddlePaddle version of the implementation, 
which is largely motivated and modified from the XLM codebase by Facebook AI Research, 
and the Transformers library by HuggingFace.  

There is also a Pytorch version, which is available upon request. 

## Usage

## Reference
If you find our work useful, please consider citing it as follows:
```
@inproceedings{yu2021cross,
  title={Cross-lingual Language Model Pretraining for Retrieval},
  author={Yu, Puxuan and Fei, Hongliang and Li, Ping},
  booktitle={Proceedings of the Web Conference 2021},
  pages={1029--1039},
  year={2021}
}
```
